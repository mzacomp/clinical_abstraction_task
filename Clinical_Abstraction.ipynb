{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Abstraction Task - LLM-Based Clinical Insights \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.82.0)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/mahsanzare/Library/Python/3.10/lib/python/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (2.11.5)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mahsanzare/Library/Python/3.10/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/mahsanzare/Library/Python/3.10/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mahsanzare/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.10 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Libraries \n",
    "%pip install openai pandas python-dotenv openpyxl scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements \n",
    "import os, json, pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "#Load ENV Variables\n",
    "load_dotenv(); \n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patient: John Smith, 58-year-old male\\nMedical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Patient: Linda Green, 45-year-old female\\nMedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Patient: Michael Brown, 62-year-old male\\nMedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Patient: Sarah Johnson, 50-year-old female\\nMe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Patient: Carlos Ramirez, 55-year-old male\\nMed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Patient: Rebecca Lee, 29-year-old female\\nMedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Patient: Thomas Wilson, 67-year-old male\\nMedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Patient: Emily Dawson, 36-year-old female\\nMed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Patient: Robert Kim, 48-year-old male\\nMedical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Patient: Diane Carter, 60-year-old female\\nMed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                note\n",
       "0  Patient: John Smith, 58-year-old male\\nMedical...\n",
       "1  Patient: Linda Green, 45-year-old female\\nMedi...\n",
       "2  Patient: Michael Brown, 62-year-old male\\nMedi...\n",
       "3  Patient: Sarah Johnson, 50-year-old female\\nMe...\n",
       "4  Patient: Carlos Ramirez, 55-year-old male\\nMed...\n",
       "5  Patient: Rebecca Lee, 29-year-old female\\nMedi...\n",
       "6  Patient: Thomas Wilson, 67-year-old male\\nMedi...\n",
       "7  Patient: Emily Dawson, 36-year-old female\\nMed...\n",
       "8  Patient: Robert Kim, 48-year-old male\\nMedical...\n",
       "9  Patient: Diane Carter, 60-year-old female\\nMed..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in Excel File \n",
    "df = pd.read_excel(\"synthetic_clinical_notes.xlsx\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "\n",
    "You are a medical assistant who is tasked with extracting structured clinical insights from unstructured clinical notes. From these notes, extract these insights into a \n",
    "JSON output, following this exact schema: \n",
    "\n",
    "{\n",
    "  \"patient_name\": \"\",\n",
    "  \"age\": \"\",\n",
    "  \"gender\": \"\",\n",
    "  \"disease\": \"\",\n",
    "  \"chief_complaint\": \"\",\n",
    "  \"vital_signs\": {},\n",
    "  \"symptoms\": [],\n",
    "  \"medications\": [],\n",
    "  \"lab_results\": {},\n",
    "  \"assessment_plan\": \"\"\n",
    "  \"summarization_main_findings\": \"Generate a one-sentence summary for each patient highlighting their key risk factors.\"\n",
    "}\n",
    "\n",
    "Important Guidelines: \n",
    "- Do NOT include any additional explanation or text outside of the JSON.\n",
    "- If a field is not mentioned in the note, use an empty string (\"\"), an empty object ({}), or an empty list ([]) as appropriate.\n",
    "- Do not infer or hallucinate information that is not explicitly stated in the note.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt = \"\"\"\n",
    "You are a strict clinical note evaluator.\n",
    "\n",
    "Your task is to assess whether a structured JSON extraction from a clinical note is accurate, complete, and hallucination-free.\n",
    "\n",
    "Return ONLY a valid JSON object using the exact format below. Do NOT include any markdown formatting, code blocks, triple backticks, or explanations.\n",
    "\n",
    "Schema:\n",
    "\n",
    "{\n",
    "  \"overall_verdict\": \"Generate a two-three sentence summary on how accurate the structured JSON extraction is to the ground truth. Explain and state what was wrong and right, and how it can be improved if need be.\",\n",
    "  \"field_verdicts\": {\n",
    "    \"patient_name\": <verdict>,\n",
    "    \"age\": <verdict>,\n",
    "    \"gender\": <verdict>,\n",
    "    \"disease\": <verdict>,\n",
    "    \"chief_complaint\": <verdict>,\n",
    "    \"vital_signs\": <verdict>,\n",
    "    \"symptoms\": <verdict>,\n",
    "    \"medications\": <verdict>,\n",
    "    \"lab_results\": <verdict>,\n",
    "    \"assessment_plan\": <verdict>,\n",
    "    \"summarization_main_findings\": <verdict>\n",
    "  },\n",
    "  \"evidence\": {\n",
    "    \"patient_name\": \"<quote from note or ''>\",\n",
    "    \"age\": \"...\",\n",
    "    \"gender\": \"...\",\n",
    "    \"disease\": \"...\",\n",
    "    \"chief_complaint\": \"...\",\n",
    "    \"vital_signs\": \"...\",\n",
    "    \"symptoms\": \"...\",\n",
    "    \"medications\": \"...\",\n",
    "    \"lab_results\": \"...\",\n",
    "    \"assessment_plan\": \"...\",\n",
    "    \"summarization_main_findings\": \"...\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Valid values for each <verdict> are:\n",
    "- \"correct\" - the field strongly and/or exactly matches the content of the note, or is correctly left empty when not mentioned.\n",
    "- \"missing\" - the field is mentioned in the note but missing from the extracted JSON.\n",
    "- \"hallucinated\" - the field includes non-empty content that is not present in the note.\n",
    "- \"incorrect\" - the field is present in both but contains inaccurate or mismatched content.\n",
    "\n",
    "Note: If a field is *not* present in the note and is returned as an empty string (\"\"), object ({}), or list ([]), this should be considered \"correct\", not \"hallucinated\".\n",
    "\n",
    "Special evaluation for \"summarization_main_findings\":\n",
    "\n",
    "This field should summarize the patient's key risk factors or medical concerns from the note in one sentence. It does not need to match the text verbatim but should:\n",
    "- Include the most important clinical findings (e.g., high A1C, fatigue, hypertension).\n",
    "- Avoid introducing new or unrelated conditions.\n",
    "\n",
    "Valid verdicts for this field are:\n",
    "- \"accurate\" - the summary correctly reflects key findings in the note.\n",
    "- \"inaccurate\" - the summary misses, distorts, or overstates the clinical findings.\n",
    "- \"irrelevant\" - the summary is unrelated to the note or nonsensical.\n",
    "\n",
    "Do NOT label this field as \"hallucinated\" based on wording differences alone.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling on Main LLM and Judge LLM\n",
    "\n",
    "#Main LLM\n",
    "\n",
    "def call_clinical_note_model(note, temperature=0.1, max_tokens=3000):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": note}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    return json.loads(content)\n",
    "\n",
    "\n",
    "\n",
    "#Judge LLM\n",
    "def call_judge_model(note, extracted_json, max_tokens=3000, temperature=0.1):\n",
    "    prompt =  f\"\"\"{judge_prompt}\n",
    "\n",
    "Here is the clinical note:\n",
    "\\\"\\\"\\\"{note}\\\"\\\"\\\"\n",
    "\n",
    "Here is the extracted JSON:\n",
    "{json.dumps(extracted_json, indent=2)}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content.strip()\n",
    "\n",
    "    #  Unwrap triple backtick blocks \n",
    "    if content.startswith(\"```json\") or content.startswith(\"```\"):\n",
    "        content = content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    #  Handle extra quotation marks\n",
    "    if content.startswith('\"') and content.endswith('\"'):\n",
    "        content = json.loads(content)  # unwrap the string first\n",
    "        print(\" Cleaned judge response:\")\n",
    "        print(content)\n",
    "\n",
    "\n",
    "    return json.loads(content)\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"patient_name\": \"Emily Dawson\",\n",
      "  \"age\": \"36\",\n",
      "  \"gender\": \"female\",\n",
      "  \"disease\": \"Type 2 Diabetes\",\n",
      "  \"chief_complaint\": \"Increased thirst and frequent urination for the past two weeks.\",\n",
      "  \"vital_signs\": {},\n",
      "  \"symptoms\": [\n",
      "    \"increased thirst\",\n",
      "    \"frequent urination\",\n",
      "    \"mild anxiety\"\n",
      "  ],\n",
      "  \"medications\": [\n",
      "    \"Metformin 500 mg once daily\"\n",
      "  ],\n",
      "  \"lab_results\": {\n",
      "    \"A1C\": \"9.0%\"\n",
      "  },\n",
      "  \"assessment_plan\": \"Stress the importance of medication adherence. Consider adding a short-acting insulin if glucose remains uncontrolled. Advise adopting consistent meal times.\",\n",
      "  \"summarization_main_findings\": \"Emily has poorly controlled Type 2 Diabetes with an A1C of 9.0%, is non-compliant with her medication, and is experiencing stress from a new job.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "note = df[\"note\"].iloc[7]\n",
    "response = call_clinical_note_model(note)\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final judge audit:\n",
      "{\n",
      "  \"overall_verdict\": \"The structured JSON extraction is mostly accurate, capturing the key details from the clinical note. However, it misses the mention of the patient's stressful new job and the illness of her dog, which are relevant to her current condition. Including these details would improve the completeness of the extraction.\",\n",
      "  \"field_verdicts\": {\n",
      "    \"patient_name\": \"correct\",\n",
      "    \"age\": \"correct\",\n",
      "    \"gender\": \"correct\",\n",
      "    \"disease\": \"correct\",\n",
      "    \"chief_complaint\": \"correct\",\n",
      "    \"vital_signs\": \"correct\",\n",
      "    \"symptoms\": \"correct\",\n",
      "    \"medications\": \"correct\",\n",
      "    \"lab_results\": \"correct\",\n",
      "    \"assessment_plan\": \"correct\",\n",
      "    \"summarization_main_findings\": \"accurate\"\n",
      "  },\n",
      "  \"evidence\": {\n",
      "    \"patient_name\": \"Emily Dawson\",\n",
      "    \"age\": \"36-year-old\",\n",
      "    \"gender\": \"female\",\n",
      "    \"disease\": \"Type 2 Diabetes\",\n",
      "    \"chief_complaint\": \"Increased thirst and frequent urination for the past two weeks.\",\n",
      "    \"vital_signs\": \"\",\n",
      "    \"symptoms\": \"Increased thirst and frequent urination for the past two weeks. Currently experiencing mild anxiety.\",\n",
      "    \"medications\": \"Prescribed Metformin 500 mg once daily\",\n",
      "    \"lab_results\": \"A1C from last visit was 9.0%\",\n",
      "    \"assessment_plan\": \"Stress the importance of medication adherence. Consider adding a short-acting insulin if glucose remains uncontrolled. Advise adopting consistent meal times.\",\n",
      "    \"summarization_main_findings\": \"Diagnosed with Type 2 Diabetes four years ago, not always compliant with medication. A1C from last visit was 9.0%.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Grab one note\n",
    "note = df[\"note\"].iloc[7]\n",
    "\n",
    "# Extract structured output\n",
    "extracted = call_clinical_note_model(note)\n",
    "\n",
    "# Judge the output\n",
    "if \"error\" not in extracted:\n",
    "    audit = call_judge_model(note, extracted)\n",
    "    print(\"\\n Final judge audit:\")\n",
    "    print(json.dumps(audit, indent=2))\n",
    "else:\n",
    "    print(\" Extraction failed:\")\n",
    "    print(extracted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Extraction + Judge \n",
    "\n",
    "audits = []\n",
    "extracted_outputs = []\n",
    "\n",
    "for note in df[\"note\"]:\n",
    "    try:\n",
    "        extracted = call_clinical_note_model(note)\n",
    "        audit = call_judge_model(note, extracted)\n",
    "        extracted_outputs.append(extracted)\n",
    "        audits.append(audit)\n",
    "    except Exception as e:\n",
    "        extracted_outputs.append({\"error\": str(e)})\n",
    "        audits.append({\"error\": str(e)})\n",
    "\n",
    "# Save Results\n",
    "with open(\"predicted_clinical_insights.json\", \"w\") as f:\n",
    "    json.dump(extracted_outputs, f, indent=2)\n",
    "\n",
    "with open(\"judge_LLM_results.json\", \"w\") as f:\n",
    "    json.dump(audits, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for Each Field:\n",
      "\n",
      "patient_name              → Precision: 1.00 | Recall: 1.00 | F1: 1.00 | Accuracy: 1.00\n",
      "age                       → Precision: 0.95 | Recall: 1.00 | F1: 0.97 | Accuracy: 0.95\n",
      "gender                    → Precision: 1.00 | Recall: 1.00 | F1: 1.00 | Accuracy: 1.00\n",
      "disease                   → Precision: 0.95 | Recall: 1.00 | F1: 0.97 | Accuracy: 0.95\n",
      "chief_complaint           → Precision: 0.89 | Recall: 0.89 | F1: 0.89 | Accuracy: 0.80\n",
      "vital_signs               → Precision: 1.00 | Recall: 0.85 | F1: 0.92 | Accuracy: 0.85\n",
      "symptoms                  → Precision: 0.76 | Recall: 0.81 | F1: 0.79 | Accuracy: 0.65\n",
      "medications               → Precision: 0.95 | Recall: 1.00 | F1: 0.97 | Accuracy: 0.95\n",
      "lab_results               → Precision: 0.85 | Recall: 1.00 | F1: 0.92 | Accuracy: 0.85\n",
      "assessment_plan           → Precision: 1.00 | Recall: 1.00 | F1: 1.00 | Accuracy: 1.00\n",
      "summarization_main_findings → Precision: 1.00 | Recall: 0.95 | F1: 0.97 | Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "#Some Metrics \n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Your fields\n",
    "standard_fields = [\n",
    "    \"patient_name\", \"age\", \"gender\", \"disease\", \"chief_complaint\",\n",
    "    \"vital_signs\", \"symptoms\", \"medications\", \"lab_results\", \"assessment_plan\"\n",
    "]\n",
    "\n",
    "# Special handling for summarization\n",
    "def extract_labels(field_name, audits):\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for audit in audits:\n",
    "        if \"field_verdicts\" not in audit:\n",
    "            continue\n",
    "\n",
    "        verdict = audit[\"field_verdicts\"].get(field_name, \"\")\n",
    "\n",
    "        if field_name == \"summarization_main_findings\":\n",
    "            if verdict == \"accurate\":\n",
    "                y_true.append(1)\n",
    "                y_pred.append(1)\n",
    "            elif verdict == \"inaccurate\":\n",
    "                y_true.append(1)\n",
    "                y_pred.append(0)\n",
    "            elif verdict == \"irrelevant\":\n",
    "                y_true.append(0)\n",
    "                y_pred.append(1)\n",
    "        else:\n",
    "            if verdict == \"correct\":\n",
    "                y_true.append(1)\n",
    "                y_pred.append(1)\n",
    "            elif verdict == \"missing\":\n",
    "                y_true.append(1)\n",
    "                y_pred.append(0)\n",
    "            elif verdict in (\"hallucinated\", \"incorrect\"):\n",
    "                y_true.append(0)\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                continue  # Skip unknown/empty verdicts\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "# Evaluate all fields\n",
    "all_fields = standard_fields + [\"summarization_main_findings\"]\n",
    "\n",
    "print(\"Evaluation Metrics for Each Field:\\n\")\n",
    "\n",
    "for field in all_fields:\n",
    "    y_true, y_pred = extract_labels(field, audits)\n",
    "    if y_true:\n",
    "        p = precision_score(y_true, y_pred)\n",
    "        r = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        print(f\"{field:25} → Precision: {p:.2f} | Recall: {r:.2f} | F1: {f1:.2f} | Accuracy: {acc:.2f}\")\n",
    "    else:\n",
    "        print(f\"{field:25} → No data available\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
