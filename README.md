Clinical Abstraction Task 

LLM-Prompt Q&A was the methodology selected to extract clinical variables from the synthetic clinical note data. I chose this methodology because Large Language Models have the capability to logically reason with the content and context within clinical text, while Named Entity Recognition and Rule-Based Extraction do not have that capability. I adopted a Judge LLM framework within this task where GPT-4o would assess the accuracy of the outputs made by the test model, GPT-4o-mini. I utilized the OpenAI framework for prompt engineering and model calling. With my prompt engineering strategy, I explicitly asked the model within the system prompt to not hallucinate information that was not present in the clinical note. I directed it to leave empty brackets if the information was not present to prevent the test model from hallucinating information. The JSON object list that I requested from the test model formats the extraction output in order to enforce the test model to extract the explicit clinical variables and to deter from extracting irrelevant information. The Judge LLM would then assess how accurate the test LLM’s output was and would provide an overall verdict on the test LLM’s performance. Additionally, I utilized evaluation metrics- accuracy, precision, recall, F1-score- to further understand how well the test model performed. Overall, the test LLM performed well and extracted the requested variables. 

Limitations & Potential Improvements: 
Stricter prompting with Judge LLM - The summarization of main findings and assessment plan variables were determined to be very accurate across all the metrics, but I would like the Judge LLM to be more strict with these textual summarizations
Include NER as well - If I had more time, I would have also utilized NER as a twofold way to ensure the correct variables are being extracted. I would have attempted entity-linking and how that could have improved the LLM’s outputs as well. 
Data Analytics Pipeline & Dashboard - I would have automated the evaluation metrics to perform automatic metric analysis of the LLM outputs. I would potentially utilize the insights found with the LLM to be visually portrayed with a data analytics dashboard. 
